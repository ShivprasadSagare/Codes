{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenizer.add_tokens(['<H>', '<R>', '<T>'])    # try with special tokens later\n",
    "        self.data_path = data_path\n",
    "        self.data = []\n",
    "        with open(self.data_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for entry in data['entries']:\n",
    "                for value in entry.values():\n",
    "                    triple_set = []\n",
    "                    for triple in value['modifiedtripleset']:\n",
    "                        triple_set.append('<H>')\n",
    "                        triple_set.append(triple['subject'])\n",
    "                        triple_set.append('<R>')\n",
    "                        triple_set.append(triple['property'])\n",
    "                        triple_set.append('<T>')\n",
    "                        triple_set.append(triple['object'])\n",
    "                    for lex in value['lexicalisations']:\n",
    "                        self.data.append((' '.join(triple_set), lex['lex']))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triple_set = self.data[idx][0]\n",
    "        lex = self.data[idx][1]\n",
    "        source = self.tokenizer(triple_set, return_tensors='pt', padding='max_length', truncation=True)\n",
    "        target = self.tokenizer(lex, return_tensors='pt', padding='max_length', truncation=True)\n",
    "\n",
    "        return {'source_input_ids': source['input_ids'].squeeze(), 'source_attention_mask': source['attention_mask'].squeeze(), 'target_input_ids': target['input_ids'].squeeze(), 'target_attention_mask': target['attention_mask'].squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, tokenizer, optimizer):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        source_input_ids = batch['source_input_ids'].to(device)\n",
    "        source_attention_mask = batch['source_attention_mask'].to(device)\n",
    "        labels = batch['target_input_ids'].to(device)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        outputs = model(source_input_ids, attention_mask=source_attention_mask, labels=labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        wandb.log({'loss': loss})\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, tokenizer):\n",
    "    actual, preds = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            source_input_ids = batch['source_input_ids'].to(device)\n",
    "            source_attention_mask = batch['source_attention_mask'].to(device)\n",
    "            outputs = model.generate(source_input_ids, attention_mask=source_attention_mask)\n",
    "            preds.append([tokenizer.decode(output, skip_special_tokens=True) for output in outputs])\n",
    "            actual.append([tokenizer.decode(batch['target_input_ids'][i], skip_special_tokens=True) for i in range(batch['target_input_ids'].shape[0])])\n",
    "    return actual, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshivprasad\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/shivprasad/t5-webnlg/runs/3p2itzyf\" target=\"_blank\">autumn-puddle-8</a></strong> to <a href=\"https://wandb.ai/shivprasad/t5-webnlg\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    wandb.init(project='t5-webnlg')\n",
    "    wandb.WANDB_NOTEBOOK_NAME = 't5-webnlg'\n",
    "    config = wandb.config\n",
    "    config.epochs = 1\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    \n",
    "    train_data_path = './train.json'\n",
    "    train_dataset = MyDataset(train_data_path, tokenizer)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    val_data_path = './dev.json'\n",
    "    val_dataset = MyDataset(val_data_path, tokenizer)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    wandb.watch(model, log='all')\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        train(model, train_dataloader, tokenizer, optimizer)\n",
    "\n",
    "    actual, preds = validate(model, val_dataloader, tokenizer)\n",
    "    json.dump({'actual': actual, 'preds': preds}, open('preds.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bhajji comes from the country of India, where two of the leaders are, T. S. Thakur and Narendra Modi. --- Bhajji is a leader in India, which is led by Narendra Mod\n",
      "********************\n",
      "Bhajji originates from India, where two of the leaders are Narendra Modi and T.S. Thakur. --- Bhajji is a leader in India, which is led by Narendra Mod\n",
      "********************\n",
      "The dish bhajji originates in India where T.S. Thakur and Narendra Modi are leaders. --- Bhajji is a leader in India, which is led by Narendra Mod\n",
      "********************\n",
      "Bhajji originates from the Karnataka region and the main ingredients are vegetables and gram flour. --- Bhajji is a main ingredient in Karnataka. It is\n",
      "********************\n",
      "The main ingredients in Bhajji are gram flour and vegetables, this comes from the Karnataka region. --- Bhajji is a main ingredient in Karnataka. It is\n",
      "********************\n",
      "Bhajji are found in the region of Karnataka, its main ingredients are gram flour and vegetables. --- Bhajji is a main ingredient in Karnataka. It is\n",
      "********************\n",
      "The binignit dish can be found in the Philippines. Two of the main ingredients in it are banana and sweet potato. --- Binignit is a popular ingredient in the Philippines. It is a sweet potato and\n",
      "********************\n",
      "The main ingredients of Binignit are banana and sweet potatoes, and it can be found in the Philippines. --- Binignit is a popular ingredient in the Philippines. It is a sweet potato and\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "with open('preds.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    actual = data['actual']\n",
    "    preds = data['preds']\n",
    "    for id, (a, p) in enumerate(zip(actual, preds)):\n",
    "        if id==301:\n",
    "            for i in range(8):\n",
    "                print(a[i], '---', p[i])\n",
    "                print('*'*20)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd0d782713032c239be331c9ea6d054a3c886f04ae6d19082ca5dd54216e8bc8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
